# Autograd Engine & Neural Network Library

A minimal, PyTorch-inspired deep learning framework built from scratch using **Python** and **NumPy**. This project demonstrates a foundational understanding of **automatic differentiation**, **neural network architectures**, and **low-level numerical computation**.

---

## ğŸš€ Features

- **ğŸ” Dynamic Computation Graph**  
  Tracks operations on `Tensor` objects and enables flexible construction of computation graphs for automatic differentiation.

- **ğŸ§® Reverse-Mode Autograd**  
  Implements efficient backpropagation using **topological sorting** of the computation graph.

- **ğŸ§± Modular Neural Network API (`nn.Module`)**  
  - PyTorch-like `Module` base class with automatic submodule registration  
  - Core layer implementations:
    - `Linear`: Fully connected layer  
    - `ReLU`: Rectified Linear Unit  
    - `Conv1D`: 1D convolution with configurable kernel size, stride, and padding

- **ğŸ“‰ Loss Functions**  
  - `MSELoss` (Mean Squared Error)  
  - `CrossEntropyLoss`

- **âš™ï¸ Optimizers**  
  - `SGD` (Stochastic Gradient Descent)  
  - `Adam` (Adaptive Moment Estimation)

- **ğŸ“¦ Data Handling**  
  - Custom `DataLoader` for batching and shuffling datasets efficiently

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ base/
â”‚   â””â”€â”€ tensor.py            # Core Tensor class with autograd & graph operations
â”‚
â”œâ”€â”€ nn.py                    # Layer implementations (Module, Linear, Conv1D, ReLU, Loss, Optimizers)
â”œâ”€â”€ DataLoader.py            # Minimal DataLoader for batching and shuffling
â”œâ”€â”€ main.py                  # Model training & evaluation entry point
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ ecg5000_cnn.py       # 1D CNN training example on ECG5000 dataset
â”‚   â””â”€â”€ mnist_mlp.py         # MLP training example on MNIST
â”‚
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # Project documentation
```

---

## âš¡ Quick Start

### ğŸ”§ Installation

```bash
pip install -r requirements.txt
```

### ğŸ§ª Running Examples

**Train a CNN on ECG5000**
```bash
python -m examples.ecg5000_cnn
```

**Train an MLP on MNIST**
```bash
python -m examples.mnist_mlp
```

---

## ğŸ”­ Future Enhancements

- [ ] Add `Conv2D` layer and related support
- [ ] Implement tensor broadcasting and advanced element-wise ops
- [ ] GPU acceleration via C++/CUDA backend
- [ ] Add support for dropout and batch normalization
- [ ] Save/load model checkpoints

---

## ğŸ§  Why This Project?

This framework was built to deeply understand:
- How **reverse-mode autodiff** and computation graphs work  
- How layers are modularized and composed in modern DL libraries  
- The internal mechanics of optimizers and backpropagation  
- The relationship between low-level numerical ops and high-level abstraction
